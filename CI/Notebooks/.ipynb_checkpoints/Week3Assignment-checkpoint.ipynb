{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Collective Intelligence Week 3: 12 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: \n",
    "\n",
    "__Student id(s)__ : \n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file.\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "## Make a workcloud using a package\n",
    "\n",
    "**NOTE**: the code below is not needed for the questions. You can try to run it, but installing the wordcloud package may be hard. If so, just continue. **You can also work with the HTML word clouds provided below.** There are also many tricks on the web to make much more beautiful word clouds.\n",
    "\n",
    "We used <https://github.com/amueller/word_cloud>. \n",
    "\n",
    "Note that the example below is an adaptation of his `simple.py`. In particular one must specify the path to the fonts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/constitution.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ba233e049037>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# You can download it using requests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../Data/constitution.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# open(path.join(d, 'constitution.txt')).read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#wordcloud = WordCloud(font_path='/Library/Fonts/Verdana.ttf').generate(text)  # Note that this is different from the code on the web! You must specify the font path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Open a plot of the generated image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../../Data/constitution.txt'"
     ]
    }
   ],
   "source": [
    "#from os import path\n",
    "% matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "# d = path.curdir\n",
    "\n",
    "# Read the whole text.\n",
    " \n",
    "# This text is in the folder http://maartenmarx.nl/teaching/CollectieveIntelligentie/Data \n",
    "# You can download it using requests\n",
    "\n",
    "text =  open('constitution.txt').read() # open(path.join(d, 'constitution.txt')).read()\n",
    "#wordcloud = WordCloud(font_path='/Library/Fonts/Verdana.ttf').generate(text)  # Note that this is different from the code on the web! You must specify the font path\n",
    "# Open a plot of the generated image.\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.2.1.tar.gz (165kB)\n",
      "Installing collected packages: wordcloud\n",
      "  Running setup.py install for wordcloud: started\n",
      "    Running setup.py install for wordcloud: finished with status 'error'\n",
      "    Complete output from command C:\\Users\\Elvis\\Anaconda\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\elvis\\\\appdata\\\\local\\\\temp\\\\pip-build-icmwqy\\\\wordcloud\\\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record c:\\users\\elvis\\appdata\\local\\temp\\pip-bjsns6-record\\install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-2.7\n",
      "    creating build\\lib.win-amd64-2.7\\wordcloud\n",
      "    copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-2.7\\wordcloud\n",
      "    copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-2.7\\wordcloud\n",
      "    copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-2.7\\wordcloud\n",
      "    copying wordcloud\\__init__.py -> build\\lib.win-amd64-2.7\\wordcloud\n",
      "    copying wordcloud\\stopwords -> build\\lib.win-amd64-2.7\\wordcloud\n",
      "    copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-2.7\\wordcloud\n",
      "    running build_ext\n",
      "    building 'wordcloud.query_integral_image' extension\n",
      "    error: Microsoft Visual C++ 9.0 is required (Unable to find vcvarsall.bat). Get it from http://aka.ms/vcpython27\n",
      "    \n",
      "    ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command \"C:\\Users\\Elvis\\Anaconda\\python.exe -u -c \"import setuptools, tokenize;__file__='c:\\\\users\\\\elvis\\\\appdata\\\\local\\\\temp\\\\pip-build-icmwqy\\\\wordcloud\\\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record c:\\users\\elvis\\appdata\\local\\temp\\pip-bjsns6-record\\install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in c:\\users\\elvis\\appdata\\local\\temp\\pip-build-icmwqy\\wordcloud\\\n"
     ]
    }
   ],
   "source": [
    "# Install wordcloud package \n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One possible way of smoothing the raw counts to font sizes.\n",
    "* Taken from <http://snipplr.com/view/8875/tag-cloud/>\n",
    "* Note the second, equivalent, implementation.\n",
    "* Which one do you like better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-34d9330bd515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# words is a dict with string:integer pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# http://snipplr.com/view/8875/tag-cloud/\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "words= Counter(text.split())as\n",
    "\n",
    "def gen_tags(words): # words is a dict with string:integer pairs\n",
    "    return ' '.join([('<font size=\"%d\">%s</font>'%(min(1+p*5/max(words.values()), 5), x)) for (x, p) in words.items()])\n",
    "\n",
    "def gen_tags2(words):\n",
    "    return ' '.join([('<font size=\"%d\">%s</font>'%(min(1+words[x]*5/max(words.values()), 5), x)) for x  in words])\n",
    "\n",
    "gen_tags(words)==gen_tags2(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 384),\n",
       " ('of', 287),\n",
       " ('and', 185),\n",
       " ('shall', 180),\n",
       " ('be', 124),\n",
       " ('to', 91),\n",
       " ('in', 84),\n",
       " ('or', 77),\n",
       " ('United', 54),\n",
       " ('a', 50),\n",
       " ('by', 47),\n",
       " ('any', 42),\n",
       " ('for', 41),\n",
       " ('States,', 38),\n",
       " ('The', 37),\n",
       " ('such', 33),\n",
       " ('State', 33),\n",
       " ('which', 33),\n",
       " ('may', 31),\n",
       " ('all', 30)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make and display a wordcloud\n",
    "\n",
    "Below you see a first try at making and displaying a word cloud.\n",
    "\n",
    "It looks horrible, doesn't it? Can you improve it?\n",
    "\n",
    "* Maybe make some fonts larger\n",
    "* Go to the web and find other ways of changing wordcounts to font-sizes:\n",
    "    * buckets\n",
    "    * logarithmic smoothing\n",
    "    * ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3cb461b16591>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# take the top 25 most occuring words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtopwords\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;31m## wrong implementation!!!!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# take the top 25 most occuring words\n",
    "topwords= {w:words[w] for w in words if words[w] > 10} ## wrong implementation!!!!\n",
    "\n",
    "\n",
    "cloud='<center>'+gen_tags2(topwords)+'</center>'\n",
    "\n",
    "\n",
    "HTML(cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The questions\n",
    "\n",
    "### Grading\n",
    "Each subquestion is graded with 1 point: 1: perfect, .5: soso, 0: not good\n",
    "\n",
    "* **Q1** 5 points, one for each subquestion\n",
    "* **Q2** 4 points, one for each subquestion\n",
    "* **Q3** 3 points \n",
    "\n",
    "Total 12 points maximal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Make a tag cloud of one text.\n",
    "\n",
    "You show in a number of steps the effects of tokenization, text normalization and text selection on the quality of your tag clouds.\n",
    "\n",
    "1. Make sure your tag clouds are well readable in a notebook.\n",
    "    * Max 25 tokens in the cloud\n",
    "    * the cloud should cover almost the entrire width of the notebook\n",
    "\n",
    "1. Download <http://www.un.org/en/documents/udhr/> and extract the text. \n",
    "    * This became tricky. Don't get frustrated. Think how to solve the problem. It can be done very easy. \n",
    "\n",
    "### Q1.1 Step by step make your tag cloud better.\n",
    "\n",
    "We start with the worst most stupid tag-cloud and gradually make it better.\n",
    "You show all intermediate steps.\n",
    "\n",
    "2. Show the effect of tokenization on your tag cloud. Thus show the difference between a stupid text splitter, one which also tokenizes punctuation, and a really good tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 text normalization\n",
    "\n",
    "Show (one by one, and finally in some best combination) the effects of lower casing, (porter) stemming and lemmatization.\n",
    "\n",
    "If you have further ideas, describe them, implement them and show their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 Text Selection\n",
    "\n",
    "1. Show the effect of stopword removal. \n",
    "2. Describe the stopword list that you use.\n",
    "3. Use POS tagging to select only  specific words. Argue why those words give an interesting view on that document.\n",
    "4. Can you think of further selections? Show them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.4 Your best system\n",
    "\n",
    "1. Show your best system and argue why you like it best.\n",
    "    * For that argument to work, you must first specify on what you evaluate your clouds. What is it that you are trying to improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.5 Other units \n",
    "\n",
    "Why include only words? We could also include phrases like _United Nations_ as one \"tag\" in our word/tag clouds.\n",
    "\n",
    "So do that! Think of NLTK's `bigrams()` and `colocations()` to help you creating them.\n",
    "\n",
    "1. Make a pure bigram cloud. Using of course the best preprocessing that you found for the unigrams.\n",
    "2. Come up with a way of mixing the best unigrams and bigrams and putting them into one cloud. Show it.\n",
    "Argue why these bigrams should be in and make the cloud better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Compare a text to others.\n",
    "\n",
    "Now you will use corpus comparison techniques to further improve your tag clouds. \n",
    "You show your work with two NLTK corpora: Gutenberg and state_of_the_union.\n",
    "\n",
    "## Q2.1\n",
    "Download all state of the unions from <http://stateoftheunion.onetwothree.net/texts/index.html>, extract the text and turn them into an NLTK corpus. (tip: See NLTK Chap 2.1 \"loading your own corpus\"). You may also use <http://stateoftheunion.onetwothree.net/texts/stateoftheunion1790-2015.txt.zip>\n",
    "\n",
    "Preprocess all documents according to your best performing word cloud generator. In the end, each document is now represented as a dict of term-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2 TF-IDF scoring\n",
    "\n",
    "* Create word clouds for each document using TF-IDF scoring. \n",
    "* Compare the TF-IDF cloud with your best cloud. What is the effect? \n",
    "* Consider improvements like:\n",
    "    * taking another base of the log\n",
    "    * removing words with very low frequency (eg hapaxes)\n",
    "* See <http://stateoftheunion.onetwothree.net/appendices.html#statistical> for what TF-IDF is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.3 Log likelihood scoring\n",
    "\n",
    "Use the log likelihood score G2 as described in <http://ucrel.lancs.ac.uk/llwizard.html> , and also used in <http://stateoftheunion.onetwothree.net/appendices.html#statistical> (called LLS) to make the weights of your terms.\n",
    "\n",
    "You can smooth and normalize as done in <http://stateoftheunion.onetwothree.net/appendices.html#statistical>.\n",
    "\n",
    "Again compare this to your best cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.4 Use Both\n",
    "\n",
    "As in <http://stateoftheunion.onetwothree.net/appendices.html#statistical> take the average of both techniques, normalize and smooth and create clouds again. \n",
    "\n",
    "Are they better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Gutenberg\n",
    "\n",
    "Use your best settings to create a wordcloud of the novels in the NLTK gutenberg collection.\n",
    "\n",
    "Show them and discuss their quality.\n",
    "\n",
    "Can you come up with a way of evaluating their quality? Describe it carefully in at most 10 sentences. BONUS if you also carry it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
